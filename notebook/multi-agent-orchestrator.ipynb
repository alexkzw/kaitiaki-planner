{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c161d-73c8-470a-8d9f-f298d807f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Kaitiaki Planner — Budget-aware Multi-Agent PoC (Capstone)\n",
    "# Runs baseline vs budgeter (with/without rerank), prints metrics, and saves plots/CSV.\n",
    "\n",
    "# %%\n",
    "import os, json, time, yaml, unicodedata as ud, numpy as np, pandas as pd, requests\n",
    "from pathlib import Path\n",
    "\n",
    "RETR = \"http://localhost:8001\"\n",
    "ORCH = \"http://localhost:8000\"\n",
    "\n",
    "# Check services\n",
    "print(requests.get(f\"{RETR}/healthz\").status_code, requests.get(f\"{RETR}/healthz\").text)\n",
    "print(requests.get(f\"{ORCH}\").status_code, requests.get(f\"{ORCH}\").text)\n",
    "\n",
    "# %%\n",
    "# Load/build corpus for ingest\n",
    "corpus = json.load(open(\"../data/corpus.json\",\"r\",encoding=\"utf-8\"))\n",
    "DOC_TEXT = {d[\"id\"]: d[\"text\"] for d in corpus}\n",
    "ing = requests.post(f\"{RETR}/ingest\", json={\"docs\": corpus}, timeout=120)\n",
    "print(\"Ingest:\", ing.status_code, ing.json())\n",
    "\n",
    "# %%\n",
    "# --- Gold offsets (Unicode/normalization safe) ---\n",
    "eval_yaml = yaml.safe_load(open(\"../eval/tasks.yaml\",\"r\",encoding=\"utf-8\"))\n",
    "\n",
    "def nfc(x): return ud.normalize(\"NFC\", x)\n",
    "\n",
    "def find_offsets(doc_id, snippet):\n",
    "    text = DOC_TEXT[doc_id]\n",
    "    T = nfc(text).lower()\n",
    "    S = nfc(snippet).lower()\n",
    "    i = T.find(S)\n",
    "    if i == -1:\n",
    "        preview = nfc(text)[:80].replace(\"\\n\",\" \")\n",
    "        raise ValueError(f\"[gold snippet missing] doc_id={doc_id} | snippet='{snippet}' | doc_starts='{preview}…'\")\n",
    "    return i, i + len(S)\n",
    "\n",
    "eval_tasks = []\n",
    "for item in eval_yaml:\n",
    "    s, e = find_offsets(item[\"gold\"][\"doc_id\"], item[\"gold\"][\"text_snippet\"])\n",
    "    eval_tasks.append({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"query\": item[\"query\"],\n",
    "        \"lang\": item[\"lang\"],\n",
    "        \"gold_citations\": [{\"doc_id\": item[\"gold\"][\"doc_id\"], \"start\": s, \"end\": e}]\n",
    "    })\n",
    "len(eval_tasks)\n",
    "\n",
    "# %%\n",
    "# --- Grounded Correctness (IoU on spans) ---\n",
    "def grounded_correctness(pred_cites, gold, iou_thresh=0.3):\n",
    "    gs, ge = gold[\"start\"], gold[\"end\"]\n",
    "    gdoc = gold[\"doc_id\"]\n",
    "    for c in pred_cites or []:\n",
    "        if c.get(\"doc_id\") != gdoc:\n",
    "            continue\n",
    "        s, e = int(c.get(\"char_start\",-1)), int(c.get(\"char_end\",-1))\n",
    "        inter = max(0, min(e, ge) - max(s, gs))\n",
    "        union = max(ge, e) - min(gs, s)\n",
    "        if union > 0 and inter/union >= iou_thresh:\n",
    "            return 1.0\n",
    "    return 0.0\n",
    "\n",
    "# %%\n",
    "# --- Runner ---\n",
    "def run_suite(tasks, mode, use_rerank=True):\n",
    "    rows=[]\n",
    "    for t in tasks:\n",
    "        payload={\"query\":t[\"query\"], \"mode\":mode, \"lang\":t[\"lang\"], \"use_rerank\":use_rerank}\n",
    "        t0=time.time()\n",
    "        try:\n",
    "            r = requests.post(f\"{ORCH}/query\", json=payload, timeout=60)\n",
    "            rec = r.json()\n",
    "        except Exception as e:\n",
    "            rec = {\"response\":{\"citations\":[],\"refusal\":True}, \"metrics\":{\"total_ms\":60000, \"cost_usd\":0.0}, \"error\":str(e)}\n",
    "        dt=(time.time()-t0)*1000\n",
    "\n",
    "        metrics = rec.get(\"metrics\", {}) or {}\n",
    "        cost = metrics.get(\"cost_usd\", metrics.get(\"total_cost_usd\", 0.0))\n",
    "        lat  = metrics.get(\"total_ms\", dt)\n",
    "        cites= rec.get(\"response\",{}).get(\"citations\",[])\n",
    "        rows.append({\n",
    "            \"id\":t[\"id\"], \"lang\":t[\"lang\"], \"mode\":mode, \"use_rerank\": use_rerank,\n",
    "            \"gc\": grounded_correctness(cites, t[\"gold_citations\"][0]),\n",
    "            \"lat_ms\": lat,\n",
    "            \"cost\": cost,\n",
    "            \"refusal\": bool(rec.get(\"response\",{}).get(\"refusal\", False))\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Baseline vs Budgeter (with rerank)\n",
    "df_base = run_suite(eval_tasks, \"baseline\", use_rerank=True)\n",
    "df_budg = run_suite(eval_tasks, \"budgeter\", use_rerank=True)\n",
    "\n",
    "# Simple ablation: Budgeter WITHOUT rerank\n",
    "df_budg_norr = run_suite(eval_tasks, \"budgeter\", use_rerank=False)\n",
    "\n",
    "df = pd.concat([df_base, df_budg, df_budg_norr], ignore_index=True)\n",
    "df.head()\n",
    "\n",
    "# %%\n",
    "# --- Summaries & SLA checks ---\n",
    "def summarize(frame: pd.DataFrame):\n",
    "    if frame.empty: return {\"overall_gc\": None, \"p50_ms\": None, \"p95_ms\": None, \"mean_cost\": None}\n",
    "    return {\n",
    "        \"overall_gc\": frame[\"gc\"].mean(),\n",
    "        \"p50_ms\": float(np.percentile(frame[\"lat_ms\"], 50)),\n",
    "        \"p95_ms\": float(np.percentile(frame[\"lat_ms\"], 95)),\n",
    "        \"mean_cost\": frame[\"cost\"].mean()\n",
    "    }\n",
    "\n",
    "def fairness_gap(frame):\n",
    "    by = frame.groupby(\"lang\")[\"gc\"].mean()\n",
    "    return float(by.get(\"en\", np.nan) - by.get(\"mi\", np.nan))\n",
    "\n",
    "for name, frame in {\n",
    "    \"baseline\": df[df[\"mode\"]==\"baseline\"],\n",
    "    \"budgeter\": df[(df[\"mode\"]==\"budgeter\") & (df[\"use_rerank\"])],\n",
    "    \"budgeter_no_rerank\": df[(df[\"mode\"]==\"budgeter\") & (~df[\"use_rerank\"])]\n",
    "}.items():\n",
    "    print(name, summarize(frame), \"fairness_gap_EN-MI=\", f\"{fairness_gap(frame):.3f}\")\n",
    "\n",
    "TARGET_P95_MS = 1200\n",
    "MAX_GC_DROP   = 0.02\n",
    "\n",
    "def pick(frame, mode, use_rerank=None):\n",
    "    f = frame[frame[\"mode\"]==mode]\n",
    "    if use_rerank is True:  f = f[f[\"use_rerank\"]]\n",
    "    if use_rerank is False: f = f[~f[\"use_rerank\"]]\n",
    "    return f\n",
    "\n",
    "base   = pick(df, \"baseline\", True)\n",
    "budg   = pick(df, \"budgeter\", True)\n",
    "budgNR = pick(df, \"budgeter\", False)\n",
    "\n",
    "def s(fr):\n",
    "    return {\n",
    "        \"gc\": fr[\"gc\"].mean(),\n",
    "        \"p50\": float(np.percentile(fr[\"lat_ms\"],50)) if not fr.empty else np.nan,\n",
    "        \"p95\": float(np.percentile(fr[\"lat_ms\"],95)) if not fr.empty else np.nan,\n",
    "        \"cost_mean\": fr[\"cost\"].mean(),\n",
    "        \"refusal_rate\": fr[\"refusal\"].mean()\n",
    "    }\n",
    "\n",
    "S_base, S_budg, S_budgNR = s(base), s(budg), s(budgNR)\n",
    "print(\"BASE   :\", S_base)\n",
    "print(\"BUDGET :\", S_budg)\n",
    "print(\"BUDG_NR:\", S_budgNR)\n",
    "\n",
    "gc_drop = (S_base[\"gc\"] - S_budg[\"gc\"]) if S_base[\"gc\"] is not None else np.nan\n",
    "meets_p95 = S_budg[\"p95\"] <= TARGET_P95_MS\n",
    "meets_gc  = (gc_drop <= MAX_GC_DROP) if not np.isnan(gc_drop) else False\n",
    "\n",
    "print(f\"\\nACCEPTANCE — p95<= {TARGET_P95_MS}ms? {meets_p95} | GC drop ≤ {MAX_GC_DROP*100:.0f} pts? {meets_gc}\")\n",
    "print(f\"Fairness gap (EN−MI), budgeter: {fairness_gap(budg):.3f}\")\n",
    "print(f\"Refusal-rate gap (EN−MI), budgeter: {float(budg.groupby('lang')['refusal'].mean().get('en',0.0) - budg.groupby('lang')['refusal'].mean().get('mi',0.0)):.3f}\")\n",
    "\n",
    "# %%\n",
    "# --- Plots ---\n",
    "import matplotlib.pyplot as plt\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "variants = [\"baseline\",\"budgeter\",\"budgeter_no_rerank\"]\n",
    "vals = [\n",
    "    df[df[\"mode\"]==\"baseline\"][\"gc\"].mean(),\n",
    "    df[(df[\"mode\"]==\"budgeter\") & (df[\"use_rerank\"])][\"gc\"].mean(),\n",
    "    df[(df[\"mode\"]==\"budgeter\") & (~df[\"use_rerank\"])][\"gc\"].mean()\n",
    "]\n",
    "plt.figure()\n",
    "plt.bar(variants, vals); plt.ylim(0,1)\n",
    "plt.ylabel(\"Grounded Correctness\")\n",
    "plt.title(f\"Correctness by Variant (n={len(df_base)})\")\n",
    "plt.savefig(\"figures/fig1_correctness.png\", bbox_inches=\"tight\"); plt.show()\n",
    "\n",
    "def p50(frame): return float(np.percentile(frame[\"lat_ms\"], 50)) if not frame.empty else np.nan\n",
    "def p95(frame): return float(np.percentile(frame[\"lat_ms\"], 95)) if not frame.empty else np.nan\n",
    "P50=[p50(df[df[\"mode\"]==\"baseline\"]),\n",
    "     p50(df[(df[\"mode\"]==\"budgeter\") & (df[\"use_rerank\"])]),\n",
    "     p50(df[(df[\"mode\"]==\"budgeter\") & (~df[\"use_rerank\"])])]\n",
    "P95=[p95(df[df[\"mode\"]==\"baseline\"]),\n",
    "     p95(df[(df[\"mode\"]==\"budgeter\") & (df[\"use_rerank\"])]),\n",
    "     p95(df[(df[\"mode\"]==\"budgeter\") & (~df[\"use_rerank\"])])]\n",
    "plt.figure()\n",
    "x = np.arange(len(variants))\n",
    "plt.plot(x, P50, marker='o', label='p50')\n",
    "plt.plot(x, P95, marker='o', label='p95')\n",
    "plt.xticks(x, variants); plt.ylabel(\"Latency (ms)\"); plt.title(\"Latency by Variant\"); plt.legend()\n",
    "plt.savefig(\"figures/fig2_latency.png\", bbox_inches=\"tight\"); plt.show()\n",
    "\n",
    "fg = [fairness_gap(df[df[\"mode\"]==\"baseline\"]),\n",
    "      fairness_gap(df[(df[\"mode\"]==\"budgeter\") & (df[\"use_rerank\"])]),\n",
    "      fairness_gap(df[(df[\"mode\"]==\"budgeter\") & (~df[\"use_rerank\"])])]\n",
    "plt.figure()\n",
    "plt.bar(variants, fg); plt.axhline(0, linestyle='--'); plt.ylim(-1,1)\n",
    "plt.ylabel(\"GC(EN) - GC(MI)\")\n",
    "plt.title(\"Fairness Gap (EN - MI)\")\n",
    "plt.savefig(\"figures/fig3_fairness_gap.png\", bbox_inches=\"tight\"); plt.show()\n",
    "\n",
    "# %%\n",
    "# Save raw results\n",
    "df.to_csv(\"outputs/results.csv\", index=False)\n",
    "print(\"Saved: figures/*.png and outputs/results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
