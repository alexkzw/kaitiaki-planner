{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c161d-73c8-470a-8d9f-f298d807f8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully\n",
      "Claude client initialized\n",
      "   Model: claude-sonnet-4-5-20250929\n",
      "   Budget: $5.00 USD\n",
      "   Current spend: $0.0000 USD\n",
      "Loaded corpus: 30 documents\n",
      "   English: 15\n",
      "   Māori:   15\n",
      "\n",
      "Loaded evaluation tasks: 30 tasks\n",
      "\n",
      "Task distribution:\n",
      "complexity  complex  simple\n",
      "lang                       \n",
      "en                6       9\n",
      "mi                3      12\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE TASK\n",
      "======================================================================\n",
      "ID:         en_maori_language_q1\n",
      "Query:      [EN] Which Act recognised Māori as an official language of New Zealand?\n",
      "Language:   en\n",
      "Complexity: complex\n",
      "Gold doc:   en_maori_language\n",
      "Gold span:  chars 305-328\n",
      "======================================================================\n",
      "Starting test with 5 queries...\n",
      "This will cost approximately $0.02 USD\n",
      "\n",
      "Note: Using gold documents (not retriever service)\n",
      "\n",
      "======================================================================\n",
      "TESTING CLAUDE WITH 5 QUERIES\n",
      "======================================================================\n",
      "Mode: Gold documents (testing)\n",
      "Expected cost: ~$0.020 USD\n",
      "======================================================================\n",
      "\n",
      "[1/5] en_maori_language_q1\n",
      "  Query: [EN] Which Act recognised Māori as an official language of N...\n",
      "  Lang: en, Complexity: complex\n",
      "  FAIL: GC=0.00, Cost=$0.001539, Latency=4219ms\n",
      "\n",
      "[2/5] en_matariki_q1\n",
      "  Query: [EN] What star cluster is known as Matariki in Māori culture...\n",
      "  Lang: en, Complexity: simple\n",
      "  FAIL: GC=0.00, Cost=$0.002094, Latency=3023ms\n",
      "\n",
      "[3/5] en_marae_q1\n",
      "  Query: [EN] In Polynesian societies, what is a marae?...\n",
      "  Lang: en, Complexity: simple\n",
      "  FAIL: GC=0.00, Cost=$0.002337, Latency=3696ms\n",
      "\n",
      "[4/5] en_mount_tongariro_q1\n",
      "  Query: [EN] Mount Tongariro lies within which volcanic zone?...\n",
      "  Lang: en, Complexity: simple\n",
      "  FAIL: GC=0.00, Cost=$0.001866, Latency=2959ms\n",
      "\n",
      "[5/5] en_moa_q1\n",
      "  Query: [EN] What kind of birds were moa?...\n",
      "  Lang: en, Complexity: simple\n",
      "  FAIL: GC=0.00, Cost=$0.002112, Latency=3270ms\n",
      "\n",
      "======================================================================\n",
      "TEST SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Completed: 5/5 queries\n",
      "Time: 17.2 seconds (3.4s per query)\n",
      "\n",
      "Performance:\n",
      "  Mean GC:         0.000\n",
      "  Total cost:      $0.0099 USD\n",
      "  Avg cost/query:  $0.001990 USD\n",
      "  Refusal rate:    0/5 (0.0%)\n",
      "  Avg latency:     3433ms\n",
      "\n",
      "By language:\n",
      "       gc    cost        refusal\n",
      "     mean     sum   mean     sum\n",
      "lang                            \n",
      "en    0.0  0.0099  0.002       0\n",
      "\n",
      "By complexity:\n",
      "             gc    cost  refusal\n",
      "complexity                      \n",
      "complex     0.0  0.0015        0\n",
      "simple      0.0  0.0021        0\n",
      "\n",
      "======================================================================\n",
      "CLAUDE API STATISTICS\n",
      "======================================================================\n",
      "\n",
      "==================================================\n",
      "CLAUDE API USAGE STATISTICS\n",
      "==================================================\n",
      "Requests:           5\n",
      "Total Cost:         $0.0099 USD\n",
      "Avg Cost/Request:   $0.0020 USD\n",
      "Budget Remaining:   $4.99 USD\n",
      "Input Tokens:       1,896\n",
      "Output Tokens:      284\n",
      "Total Tokens:       2,180\n",
      "==================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "MULTI-QUERY TEST PASSED!\n",
      "======================================================================\n",
      "\n",
      "Total spend so far: $0.0099 USD\n",
      "Budget remaining: $4.99 USD\n",
      "\n",
      "Issues to investigate:\n",
      "  - Low mean GC (<0.3) - check citation extraction\n",
      "\n",
      "======================================================================\n",
      "DETAILED RESULTS\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>complexity</th>\n",
       "      <th>gc</th>\n",
       "      <th>cost</th>\n",
       "      <th>refusal</th>\n",
       "      <th>answer_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en_maori_language_q1</td>\n",
       "      <td>en</td>\n",
       "      <td>complex</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>False</td>\n",
       "      <td>The Māori Language Act 1987 recognised Māori a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en_matariki_q1</td>\n",
       "      <td>en</td>\n",
       "      <td>simple</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>False</td>\n",
       "      <td>The Pleiades star cluster is known as Matariki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en_marae_q1</td>\n",
       "      <td>en</td>\n",
       "      <td>simple</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>False</td>\n",
       "      <td>Based on the passages provided:\\n\\nA marae is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en_mount_tongariro_q1</td>\n",
       "      <td>en</td>\n",
       "      <td>simple</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>False</td>\n",
       "      <td>Mount Tongariro lies within the Taupō Volcanic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en_moa_q1</td>\n",
       "      <td>en</td>\n",
       "      <td>simple</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>False</td>\n",
       "      <td>Based on the passage provided, moa were an ext...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id lang complexity   gc      cost  refusal  \\\n",
       "0   en_maori_language_q1   en    complex  0.0  0.001539    False   \n",
       "1         en_matariki_q1   en     simple  0.0  0.002094    False   \n",
       "2            en_marae_q1   en     simple  0.0  0.002337    False   \n",
       "3  en_mount_tongariro_q1   en     simple  0.0  0.001866    False   \n",
       "4              en_moa_q1   en     simple  0.0  0.002112    False   \n",
       "\n",
       "                                      answer_preview  \n",
       "0  The Māori Language Act 1987 recognised Māori a...  \n",
       "1  The Pleiades star cluster is known as Matariki...  \n",
       "2  Based on the passages provided:\\n\\nA marae is ...  \n",
       "3  Mount Tongariro lies within the Taupō Volcanic...  \n",
       "4  Based on the passage provided, moa were an ext...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 queries with GC=0 (no correct citations):\n",
      "  - en_maori_language_q1 (en, complex)\n",
      "  - en_matariki_q1 (en, simple)\n",
      "  - en_marae_q1 (en, simple)\n",
      "  - en_mount_tongariro_q1 (en, simple)\n",
      "  - en_moa_q1 (en, simple)\n",
      "\n",
      "Saved test results to: ../outputs/test_results_day1.csv\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Kaitiaki Planner - Budget-Aware RAG Evaluation\n",
    "# **Capstone Project**: Evaluating fairness in computational budget allocation\n",
    "# \n",
    "# Research Question: How does budget allocation strategy affect fairness \n",
    "# between English and Te Reo Māori queries?\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\n",
    "        \"ANTHROPIC_API_KEY not found!\\n\"\n",
    "        \"   1. Create .env file in project root\\n\"\n",
    "        \"   2. Add: ANTHROPIC_API_KEY=sk-ant-your-key\\n\"\n",
    "        \"   3. Restart kernel\"\n",
    "    )\n",
    "\n",
    "print(\"API key loaded successfully\")\n",
    "\n",
    "# %%\n",
    "# Import custom modules\n",
    "from claude_client import ClaudeClient\n",
    "from eval_utils import (\n",
    "    load_corpus,\n",
    "    load_eval_tasks,\n",
    "    grounded_correctness,\n",
    "    fairness_gap,\n",
    "    summarize_results,\n",
    "    print_summary\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Initialize Claude Client\n",
    "\n",
    "# %%\n",
    "# Initialize with $5 budget (safety limit)\n",
    "claude = ClaudeClient(\n",
    "    api_key=api_key,\n",
    "    max_spend_usd=5.0\n",
    ")\n",
    "\n",
    "print(f\"Claude client initialized\")\n",
    "print(f\"   Model: {claude.model}\")\n",
    "print(f\"   Budget: ${claude.max_spend_usd:.2f} USD\")\n",
    "print(f\"   Current spend: ${claude.total_cost_usd:.4f} USD\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Load Data\n",
    "\n",
    "# %%\n",
    "# Load corpus\n",
    "corpus = load_corpus(\"../data/corpus.json\")\n",
    "print(f\"Loaded corpus: {len(corpus)} documents\")\n",
    "print(f\"   English: {sum(1 for d in corpus if d['lang']=='en')}\")\n",
    "print(f\"   Māori:   {sum(1 for d in corpus if d['lang']=='mi')}\")\n",
    "\n",
    "# %%\n",
    "# Load evaluation tasks\n",
    "eval_tasks = load_eval_tasks(corpus, \"../eval/tasks_labeled.yaml\")\n",
    "print(f\"\\nLoaded evaluation tasks: {len(eval_tasks)} tasks\")\n",
    "\n",
    "# Show distribution\n",
    "df_dist = pd.DataFrame(eval_tasks)\n",
    "print(\"\\nTask distribution:\")\n",
    "print(df_dist.groupby(['lang', 'complexity']).size().unstack(fill_value=0))\n",
    "\n",
    "# %%\n",
    "# Show example task\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE TASK\")\n",
    "print(\"=\"*70)\n",
    "example = eval_tasks[0]\n",
    "print(f\"ID:         {example['id']}\")\n",
    "print(f\"Query:      {example['query']}\")\n",
    "print(f\"Language:   {example['lang']}\")\n",
    "print(f\"Complexity: {example['complexity']}\")\n",
    "print(f\"Gold doc:   {example['gold_citations'][0]['doc_id']}\")\n",
    "print(f\"Gold span:  chars {example['gold_citations'][0]['start']}-{example['gold_citations'][0]['end']}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Test with Multiple Queries (Task 3)\n",
    "\n",
    "# %%\n",
    "def test_claude_multiple_queries(n_queries=5, use_retriever=False):\n",
    "    \"\"\"\n",
    "    Test Claude with first N queries from evaluation set.\n",
    "    \n",
    "    Args:\n",
    "        n_queries: Number of queries to test (default: 5)\n",
    "        use_retriever: If True, call retriever service. If False, use gold docs directly.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \n",
    "    Expected cost: ~$0.015-0.025 for 5 queries\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TESTING CLAUDE WITH {n_queries} QUERIES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Mode: {'Retriever service' if use_retriever else 'Gold documents (testing)'}\")\n",
    "    print(f\"Expected cost: ~${n_queries * 0.004:.3f} USD\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use first n_queries from eval_tasks\n",
    "    for i, task in enumerate(eval_tasks[:n_queries], 1):\n",
    "        task_id = task['id']\n",
    "        query = task['query']\n",
    "        lang = task['lang']\n",
    "        complexity = task['complexity']\n",
    "        \n",
    "        print(f\"\\n[{i}/{n_queries}] {task_id}\")\n",
    "        print(f\"  Query: {query[:60]}...\")\n",
    "        print(f\"  Lang: {lang}, Complexity: {complexity}\")\n",
    "        \n",
    "        try:\n",
    "            # Get gold document\n",
    "            gold_doc_id = task['gold_citations'][0]['doc_id']\n",
    "            gold_doc = next((d for d in corpus if d['id'] == gold_doc_id), None)\n",
    "            \n",
    "            if not gold_doc:\n",
    "                print(f\"  Warning: Gold document not found: {gold_doc_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Create passage from gold document\n",
    "            passages = [{\n",
    "                \"doc_id\": gold_doc[\"id\"],\n",
    "                \"text\": gold_doc[\"text\"][:1000],\n",
    "                \"char_start\": 0,\n",
    "                \"char_end\": min(1000, len(gold_doc[\"text\"]))\n",
    "            }]\n",
    "            \n",
    "            # Generate answer with Claude\n",
    "            t0 = time.time()\n",
    "            response = claude.generate_answer(\n",
    "                query=query,\n",
    "                passages=passages[:3],\n",
    "                max_tokens=150\n",
    "            )\n",
    "            lat_ms = (time.time() - t0) * 1000\n",
    "            \n",
    "            # Calculate grounded correctness\n",
    "            gc = grounded_correctness(\n",
    "                response['citations'],\n",
    "                task['gold_citations'][0]\n",
    "            )\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                \"id\": task_id,\n",
    "                \"lang\": lang,\n",
    "                \"complexity\": complexity,\n",
    "                \"gc\": gc,\n",
    "                \"cost\": response[\"cost_usd\"],\n",
    "                \"refusal\": response[\"refusal\"],\n",
    "                \"lat_ms\": lat_ms,\n",
    "                \"input_tokens\": response[\"usage\"][\"input_tokens\"],\n",
    "                \"output_tokens\": response[\"usage\"][\"output_tokens\"],\n",
    "                \"answer_preview\": response[\"answer\"][:80]\n",
    "            }\n",
    "            \n",
    "            test_results.append(result)\n",
    "            \n",
    "            # Print status\n",
    "            status = \"PASS\" if gc > 0 else \"FAIL\"\n",
    "            refusal_str = \" [REFUSAL]\" if response[\"refusal\"] else \"\"\n",
    "            print(f\"  {status}: GC={gc:.2f}, Cost=${response['cost_usd']:.6f}, \"\n",
    "                  f\"Latency={lat_ms:.0f}ms{refusal_str}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {str(e)[:60]}\")\n",
    "            continue\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not test_results:\n",
    "        print(\"No queries completed successfully\")\n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_test = pd.DataFrame(test_results)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nCompleted: {len(test_results)}/{n_queries} queries\")\n",
    "    print(f\"Time: {elapsed:.1f} seconds ({elapsed/len(test_results):.1f}s per query)\")\n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  Mean GC:         {df_test['gc'].mean():.3f}\")\n",
    "    print(f\"  Total cost:      ${df_test['cost'].sum():.4f} USD\")\n",
    "    print(f\"  Avg cost/query:  ${df_test['cost'].mean():.6f} USD\")\n",
    "    print(f\"  Refusal rate:    {df_test['refusal'].sum()}/{len(df_test)} ({df_test['refusal'].mean():.1%})\")\n",
    "    print(f\"  Avg latency:     {df_test['lat_ms'].mean():.0f}ms\")\n",
    "    \n",
    "    # By language\n",
    "    print(\"\\nBy language:\")\n",
    "    lang_stats = df_test.groupby('lang').agg({\n",
    "        'gc': 'mean',\n",
    "        'cost': ['sum', 'mean'],\n",
    "        'refusal': 'sum'\n",
    "    }).round(4)\n",
    "    print(lang_stats)\n",
    "    \n",
    "    # By complexity\n",
    "    if 'complexity' in df_test.columns and df_test['complexity'].notna().all():\n",
    "        print(\"\\nBy complexity:\")\n",
    "        comp_stats = df_test.groupby('complexity').agg({\n",
    "            'gc': 'mean',\n",
    "            'cost': 'mean',\n",
    "            'refusal': 'sum'\n",
    "        }).round(4)\n",
    "        print(comp_stats)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CLAUDE API STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    claude.print_stats()\n",
    "    \n",
    "    return df_test\n",
    "\n",
    "# %%\n",
    "# Run test with 5 queries\n",
    "print(\"Starting test with 5 queries...\")\n",
    "print(\"This will cost approximately $0.02 USD\")\n",
    "print(\"\\nNote: Using gold documents (not retriever service)\")\n",
    "\n",
    "df_test = test_claude_multiple_queries(n_queries=5, use_retriever=False)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Verify Results\n",
    "\n",
    "# %%\n",
    "# Check if test passed\n",
    "if df_test is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTI-QUERY TEST PASSED!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nTotal spend so far: ${claude.total_cost_usd:.4f} USD\")\n",
    "    print(f\"Budget remaining: ${claude.max_spend_usd - claude.total_cost_usd:.2f} USD\")\n",
    "    \n",
    "    # Check for potential issues\n",
    "    issues = []\n",
    "    warnings = []\n",
    "    \n",
    "    if df_test['gc'].mean() < 0.3:\n",
    "        issues.append(\"Low mean GC (<0.3) - check citation extraction\")\n",
    "    elif df_test['gc'].mean() < 0.5:\n",
    "        warnings.append(\"Moderate GC (0.3-0.5) - could be better\")\n",
    "    \n",
    "    if df_test['refusal'].sum() > len(df_test) * 0.5:\n",
    "        issues.append(\"High refusal rate (>50%) - check passage quality\")\n",
    "    \n",
    "    if df_test['cost'].mean() > 0.01:\n",
    "        issues.append(\"High cost per query (>$0.01) - passages may be too long\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\nIssues to investigate:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    \n",
    "    if warnings:\n",
    "        print(\"\\nWarnings:\")\n",
    "        for warning in warnings:\n",
    "            print(f\"  - {warning}\")\n",
    "    \n",
    "    if not issues and not warnings:\n",
    "        print(\"\\nAll checks passed - system is working well!\")\n",
    "        print(\"Ready for full evaluation!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTI-QUERY TEST FAILED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Check errors above and debug before proceeding.\")\n",
    "\n",
    "# %%\n",
    "# View detailed results\n",
    "if df_test is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DETAILED RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', 50)\n",
    "    \n",
    "    display(df_test[['id', 'lang', 'complexity', 'gc', 'cost', 'refusal', 'answer_preview']])\n",
    "    \n",
    "    # Show failures\n",
    "    failed = df_test[df_test['gc'] == 0.0]\n",
    "    if len(failed) > 0:\n",
    "        print(f\"\\n{len(failed)} queries with GC=0 (no correct citations):\")\n",
    "        for _, row in failed.iterrows():\n",
    "            print(f\"  - {row['id']} ({row['lang']}, {row['complexity']})\")\n",
    "\n",
    "# %%\n",
    "# Save test results\n",
    "if df_test is not None:\n",
    "    output_dir = Path(\"../outputs\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    output_path = output_dir / \"test_results_day1.csv\"\n",
    "    df_test.to_csv(output_path, index=False)\n",
    "    print(f\"\\nSaved test results to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
